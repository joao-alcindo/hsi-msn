# Configurações gerais do projeto
project_name: hsi-msn
output_dir: ./output
data_root: ./../../hyspecnet11k-PCA/
resume_from: null # Pode ser 'last' para continuar do último checkpoint ou null para começar do zero.

# Configurações do treinamento
num_epochs: 20
batch_size: 64
learning_rate: 5.0e-03
weight_decay: 0.05
warmup_epochs: 5
final_lr: 1.0e-07
final_weight_decay: 0.05
alpha_ema: 0.996 # Momentum inicial para o EMA
save_freq_epochs: 50
epoch_stop_prototype: 10 # Epoch em que o treinamento dos protótipos é parado

# Parâmetros da perda (MSN Loss)
use_sinkhorn: True 
temp_anchor: 0.1
temp_target: 0.025
lambda_reg: 0.1

# Parâmetros do modelo (Vision Transformer HSI)
# Tamanhos das imagens de entrada e patches
rand_size: [128, 128, 32]
focal_size: [64, 64, 32]
patch_size: [16,16,4]
in_chans: 1 # Número de canais de entrada (1 para HSI)
embed_dim: 128 # Tamanho da dimensão do embedding
depth: 9 # Número de blocos do Transformer
num_heads: 8 # Número de cabeças de atenção
mlp_ratio: 4.0
drop_rate: 0.0
attn_drop_rate: 0.0
drop_path_rate: 0.00
trunc_init: True

# Configurações específicas da MSN
num_prototipos: 50
mask_ratio: 0.5 # Porcentagem de patches mascarados para as visões âncora
rand_views: 2 # Número de visões âncora (e.g., uma aleatória e uma focal)
focal_views: 5 # Parâmetro adicionado para consistência com o train.py


rand_crop_scale: [0.8, 1.0]
focal_crop_scale: [0.3, 0.5]
num_workers: 8
pin_memory: True

spectral_jitter_strength: 0.1 