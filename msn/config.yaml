# Configurações gerais do projeto
project_name: hsi-msn
output_dir: ./output
data_root: ./data/processed
resume_from: null # Pode ser 'last' para continuar do último checkpoint ou null para começar do zero.

# Configurações do treinamento
num_epochs: 100
batch_size: 64
learning_rate: 0.0001
weight_decay: 0.05
warmup_epochs: 10
final_lr: 1.0e-06
final_weight_decay: 0.05
alpha_ema: 0.996 # Momentum inicial para o EMA
save_freq_epochs: 50
epoch_stop_prototype: 50 # Epoch em que o treinamento dos protótipos é parado

# Parâmetros da perda (MSN Loss)
use_sinkhorn: True
temp_anchor: 0.1
temp_target: 0.025
lambda_reg: 0.01

# Parâmetros do modelo (Vision Transformer HSI)
# Tamanhos das imagens de entrada e patches
rand_size: [60, 60, 150]
focal_size: [30, 30, 150]
patch_size: [5, 5, 10]
in_chans: 1 # Número de canais de entrada (1 para HSI)
embed_dim: 768 # Tamanho da dimensão do embedding
depth: 12 # Número de blocos do Transformer
num_heads: 12 # Número de cabeças de atenção
mlp_ratio: 4.0
drop_rate: 0.1
attn_drop_rate: 0.0
drop_path_rate: 0.1
trunc_init: True

# Configurações específicas da MSN
num_prototipos: 512
tamanho_embedding: 768 # Deve ser igual a embed_dim
mask_ratio: 0.1 # Porcentagem de patches mascarados para as visões âncora
num_rand_views: 2 # Número de visões âncora (e.g., uma aleatória e uma focal)
num_focal_views: 1 # Parâmetro adicionado para consistência com o train.py